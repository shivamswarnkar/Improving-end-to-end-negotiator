/home/vvb231/nrgo2/end-to-end-negotiator-master/src/models/dialog_model.py:174: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  prob = F.softmax(logit).unsqueeze(2).expand_as(h)
/home/vvb231/nrgo2/end-to-end-negotiator-master/src/models/dialog_model.py:382: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greately increasing memory usage. To compact weights again call flatten_parameters().
  out, _ = self.reader(inpt_emb, lang_h)
/home/vvb231/nrgo2/end-to-end-negotiator-master/src/models/dialog_model.py:169: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greately increasing memory usage. To compact weights again call flatten_parameters().
  h, _ = self.sel_rnn(h, attn_h)
dataset data/negotiate/train.txt, total 687919, unks 8718, ratio 1.27%
dataset data/negotiate/val.txt, total 74653, unks 914, ratio 1.22%
dataset data/negotiate/test.txt, total 70262, unks 847, ratio 1.21%
| epoch 001 | trainloss 3.577 | trainppl 35.780 | s/epoch 21.91 | lr 1.00000000
| epoch 001 | validloss 2.621 | validppl 13.754
| epoch 001 | validselectloss 1.237 | validselectppl 3.447
| epoch 002 | trainloss 2.856 | trainppl 17.390 | s/epoch 21.64 | lr 1.00000000
| epoch 002 | validloss 2.343 | validppl 10.414
| epoch 002 | validselectloss 1.194 | validselectppl 3.300
| epoch 003 | trainloss 2.639 | trainppl 14.004 | s/epoch 21.64 | lr 1.00000000
| epoch 003 | validloss 2.150 | validppl 8.588
| epoch 003 | validselectloss 0.990 | validselectppl 2.691
| epoch 004 | trainloss 2.451 | trainppl 11.594 | s/epoch 21.61 | lr 1.00000000
| epoch 004 | validloss 2.082 | validppl 8.021
| epoch 004 | validselectloss 0.817 | validselectppl 2.265
| epoch 005 | trainloss 2.277 | trainppl 9.749 | s/epoch 21.62 | lr 1.00000000
| epoch 005 | validloss 2.008 | validppl 7.450
| epoch 005 | validselectloss 0.712 | validselectppl 2.038
| epoch 006 | trainloss 2.151 | trainppl 8.597 | s/epoch 21.63 | lr 1.00000000
| epoch 006 | validloss 1.949 | validppl 7.021
| epoch 006 | validselectloss 0.586 | validselectppl 1.796
| epoch 007 | trainloss 2.083 | trainppl 8.027 | s/epoch 21.65 | lr 1.00000000
| epoch 007 | validloss 1.899 | validppl 6.682
| epoch 007 | validselectloss 0.519 | validselectppl 1.681
| epoch 008 | trainloss 2.033 | trainppl 7.637 | s/epoch 21.60 | lr 1.00000000
| epoch 008 | validloss 1.908 | validppl 6.739
| epoch 008 | validselectloss 0.471 | validselectppl 1.601
| epoch 009 | trainloss 1.982 | trainppl 7.256 | s/epoch 21.66 | lr 1.00000000
| epoch 009 | validloss 1.897 | validppl 6.667
| epoch 009 | validselectloss 0.455 | validselectppl 1.576
| epoch 010 | trainloss 1.946 | trainppl 7.002 | s/epoch 21.59 | lr 1.00000000
| epoch 010 | validloss 1.856 | validppl 6.401
| epoch 010 | validselectloss 0.388 | validselectppl 1.474
| epoch 011 | trainloss 1.903 | trainppl 6.706 | s/epoch 21.71 | lr 1.00000000
| epoch 011 | validloss 1.847 | validppl 6.340
| epoch 011 | validselectloss 0.370 | validselectppl 1.448
| epoch 012 | trainloss 1.870 | trainppl 6.488 | s/epoch 21.64 | lr 1.00000000
| epoch 012 | validloss 1.849 | validppl 6.354
| epoch 012 | validselectloss 0.280 | validselectppl 1.324
| epoch 013 | trainloss 1.836 | trainppl 6.272 | s/epoch 21.72 | lr 1.00000000
| epoch 013 | validloss 1.831 | validppl 6.241
| epoch 013 | validselectloss 0.254 | validselectppl 1.290
| epoch 014 | trainloss 1.813 | trainppl 6.129 | s/epoch 21.71 | lr 1.00000000
| epoch 014 | validloss 1.880 | validppl 6.551
| epoch 014 | validselectloss 0.256 | validselectppl 1.292
| epoch 015 | trainloss 1.786 | trainppl 5.963 | s/epoch 21.73 | lr 1.00000000
| epoch 015 | validloss 1.827 | validppl 6.214
| epoch 015 | validselectloss 0.226 | validselectppl 1.254
| epoch 016 | trainloss 1.762 | trainppl 5.826 | s/epoch 21.72 | lr 1.00000000
| epoch 016 | validloss 1.839 | validppl 6.288
| epoch 016 | validselectloss 0.249 | validselectppl 1.283
| epoch 017 | trainloss 1.744 | trainppl 5.719 | s/epoch 21.75 | lr 1.00000000
| epoch 017 | validloss 1.799 | validppl 6.046
| epoch 017 | validselectloss 0.196 | validselectppl 1.217
| epoch 018 | trainloss 1.726 | trainppl 5.619 | s/epoch 21.68 | lr 1.00000000
| epoch 018 | validloss 1.861 | validppl 6.427
| epoch 018 | validselectloss 0.176 | validselectppl 1.193
| epoch 019 | trainloss 1.705 | trainppl 5.502 | s/epoch 21.67 | lr 1.00000000
| epoch 019 | validloss 1.791 | validppl 5.997
| epoch 019 | validselectloss 0.172 | validselectppl 1.188
| epoch 020 | trainloss 1.692 | trainppl 5.430 | s/epoch 21.63 | lr 1.00000000
| epoch 020 | validloss 1.810 | validppl 6.113
| epoch 020 | validselectloss 0.142 | validselectppl 1.153
| epoch 021 | trainloss 1.680 | trainppl 5.363 | s/epoch 21.64 | lr 1.00000000
| epoch 021 | validloss 1.830 | validppl 6.235
| epoch 021 | validselectloss 0.145 | validselectppl 1.157
| epoch 022 | trainloss 1.665 | trainppl 5.283 | s/epoch 21.64 | lr 1.00000000
| epoch 022 | validloss 1.787 | validppl 5.971
| epoch 022 | validselectloss 0.159 | validselectppl 1.172
| epoch 023 | trainloss 1.647 | trainppl 5.189 | s/epoch 21.62 | lr 1.00000000
| epoch 023 | validloss 1.787 | validppl 5.971
| epoch 023 | validselectloss 0.147 | validselectppl 1.158
| epoch 024 | trainloss 1.638 | trainppl 5.145 | s/epoch 21.62 | lr 1.00000000
| epoch 024 | validloss 1.786 | validppl 5.964
| epoch 024 | validselectloss 0.139 | validselectppl 1.149
| epoch 025 | trainloss 1.626 | trainppl 5.082 | s/epoch 21.61 | lr 1.00000000
| epoch 025 | validloss 1.797 | validppl 6.033
| epoch 025 | validselectloss 0.145 | validselectppl 1.156
| epoch 026 | trainloss 1.619 | trainppl 5.047 | s/epoch 21.64 | lr 1.00000000
| epoch 026 | validloss 1.777 | validppl 5.914
| epoch 026 | validselectloss 0.127 | validselectppl 1.135
| epoch 027 | trainloss 1.605 | trainppl 4.978 | s/epoch 21.63 | lr 1.00000000
| epoch 027 | validloss 1.761 | validppl 5.817
| epoch 027 | validselectloss 0.125 | validselectppl 1.133
| epoch 028 | trainloss 1.598 | trainppl 4.941 | s/epoch 21.68 | lr 1.00000000
| epoch 028 | validloss 1.767 | validppl 5.854
| epoch 028 | validselectloss 0.151 | validselectppl 1.164
| epoch 029 | trainloss 1.586 | trainppl 4.884 | s/epoch 21.78 | lr 1.00000000
| epoch 029 | validloss 1.767 | validppl 5.853
| epoch 029 | validselectloss 0.193 | validselectppl 1.213
| epoch 030 | trainloss 1.580 | trainppl 4.853 | s/epoch 21.80 | lr 1.00000000
| epoch 030 | validloss 1.778 | validppl 5.917
| epoch 030 | validselectloss 0.116 | validselectppl 1.123
| start annealing | best validselectloss 0.116 | best validselectppl 1.123
| epoch 031 | trainloss 1.503 | trainppl 4.495 | s/epoch 21.58 | lr 0.20000000
| epoch 031 | validloss 1.724 | validppl 5.610
| epoch 031 | validselectloss 0.119 | validselectppl 1.126
| epoch 032 | trainloss 1.481 | trainppl 4.396 | s/epoch 21.55 | lr 0.04000000
| epoch 032 | validloss 1.723 | validppl 5.603
| epoch 032 | validselectloss 0.110 | validselectppl 1.116
final selectppl 1.116
